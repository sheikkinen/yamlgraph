#!/usr/bin/env python3
"""Streaming Demo - Showcases token-by-token LLM output.

Demonstrates:
- execute_prompt_streaming() async generator
- Real-time token output to terminal
- Collecting streamed tokens

Usage:
    # Interactive streaming
    python scripts/demo_streaming.py

    # With custom prompt
    python scripts/demo_streaming.py --prompt "Tell me a short story about a robot"

    # Verification mode (no LLM, mock output)
    python scripts/demo_streaming.py --verify
"""

import argparse
import asyncio
import sys
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from yamlgraph.executor_async import execute_prompt_streaming


def print_banner(title: str) -> None:
    """Print a styled banner."""
    width = 50
    print("â”Œ" + "â”€" * width + "â”")
    print(f"â”‚ {title:<{width-1}}â”‚")
    print("â”œ" + "â”€" * width + "â”¤")


def print_footer() -> None:
    """Print footer."""
    print("â””" + "â”€" * 50 + "â”˜")


async def run_streaming_demo(
    user_prompt: str,
    verify: bool = False,
) -> str:
    """Run the streaming demo.

    Args:
        user_prompt: What to ask the LLM
        verify: If True, skip actual LLM call

    Returns:
        Full collected response
    """
    print_banner("ğŸŒŠ Streaming Demo")
    print(f"â”‚ Prompt: {user_prompt[:40]:<41}â”‚")
    print("â”‚" + " " * 50 + "â”‚")

    if verify:
        # Mock streaming for verification
        print("â”‚ [Verify mode - mock streaming]                  â”‚")
        print("â”‚" + " " * 50 + "â”‚")
        print("â”‚ Response:                                        â”‚")
        print("â”‚ ", end="")

        mock_response = "Hello! This is a mock streaming response for testing purposes."
        for char in mock_response:
            print(char, end="", flush=True)
            await asyncio.sleep(0.02)

        print()
        print("â”‚" + " " * 50 + "â”‚")
        print_footer()
        return mock_response

    # Real streaming from LLM
    print("â”‚ Streaming response:                              â”‚")
    print("â”‚" + " " * 50 + "â”‚")

    tokens_collected = []

    # Create a simple prompt YAML on the fly by using greet prompt
    # In real usage, you'd have a prompt file
    try:
        async for token in execute_prompt_streaming(
            "greet",
            variables={"name": "streaming demo user", "style": user_prompt},
            provider="mistral",
        ):
            print(token, end="", flush=True)
            tokens_collected.append(token)
    except Exception as e:
        print(f"\nâ”‚ âŒ Error: {e!s:.40}â”‚")
        print_footer()
        return ""

    full_response = "".join(tokens_collected)

    print()
    print("â”‚" + " " * 50 + "â”‚")
    print(f"â”‚ âœ… Received {len(tokens_collected)} chunks, {len(full_response)} charsâ”‚")
    print_footer()

    return full_response


async def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description="Streaming Demo")
    parser.add_argument(
        "--prompt",
        default="casual and friendly",
        help="Style for the greeting prompt",
    )
    parser.add_argument(
        "--verify",
        action="store_true",
        help="Run in verification mode (mock output)",
    )
    args = parser.parse_args()

    result = await run_streaming_demo(
        user_prompt=args.prompt,
        verify=args.verify,
    )

    if args.verify:
        # Verification check
        print("\nğŸ” Verification:")
        if len(result) > 0:
            print("  âœ… Streaming produced output")
        else:
            print("  âŒ No output received")
            sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
