# LangGraph Showcase App

A minimal, self-contained demonstration / template of a LLM pipeline using:

- **YAML Graph Configuration** - Declarative pipeline definition with schema validation
- **YAML Prompts** - Declarative prompt templates with Jinja2 support
- **Pydantic Models** - Structured LLM outputs
- **Multi-Provider LLMs** - Support for Anthropic, Mistral, and OpenAI
- **LangGraph** - Pipeline orchestration with resume support
- **SQLite** - State persistence
- **LangSmith** - Observability and tracing
- **JSON Export** - Result serialization

## Quick Start

### 1. Setup Environment

```bash
# Clone or copy the showcase directory
cd showcase

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# Install as editable package
pip install -e .

# Configure environment
cp .env.sample .env
# Edit .env with your ANTHROPIC_API_KEY
```

### 2. Run the Pipeline

```bash
# Using the CLI command (after pip install -e .)
showcase run --topic "machine learning" --style casual

# Or using python -m
python -m showcase.cli run --topic "machine learning" --style casual

# With export
showcase run -t "climate change" -s informative -w 500 --export

# Universal graph runner - run any YAML graph
showcase graph run graphs/router-demo.yaml --var message="I love this!"
showcase graph run graphs/reflexion-demo.yaml --var topic="climate change"
showcase graph run graphs/git-report.yaml --var input="What changed recently?"
showcase graph run graphs/memory-demo.yaml --var input="Show me recent commits"

# Graph utilities
showcase graph list                        # List available graphs
showcase graph info graphs/router-demo.yaml # Show graph structure
showcase graph validate graphs/*.yaml      # Validate graph schemas

# Git report demo - AI agent with shell tools
showcase git-report -q "What changed recently?"
showcase git-report -q "Summarize the test files"

# Memory demo - multi-turn conversation with state persistence
showcase memory-demo --input "Show me the last 5 commits"
showcase memory-demo --input "What about test files?" --thread mem-abc123

# Deprecated commands (raise DeprecationError with replacement suggestion)
# showcase route "hello"  # ‚Üí DeprecationError: Use 'graph run graphs/router-demo.yaml --var message=hello'

# View recent runs
showcase list-runs

# Resume a run
showcase resume --thread-id abc123

# View execution trace (requires LangSmith)
showcase trace --verbose
```

## Architecture

```
showcase/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ pyproject.toml        # Package definition with CLI entry point
‚îú‚îÄ‚îÄ requirements.txt      # Dependencies
‚îú‚îÄ‚îÄ .env.sample           # Environment template
‚îú‚îÄ‚îÄ run.py                # Simple entry point
‚îÇ
‚îú‚îÄ‚îÄ graphs/               # YAML graph definitions
‚îÇ   ‚îú‚îÄ‚îÄ showcase.yaml     # Main pipeline definition
‚îÇ   ‚îú‚îÄ‚îÄ router-demo.yaml  # Tone-based routing demo
‚îÇ   ‚îú‚îÄ‚îÄ reflexion-demo.yaml # Self-refinement loop demo
‚îÇ   ‚îî‚îÄ‚îÄ git-report.yaml   # AI agent demo with shell tools
‚îÇ
‚îú‚îÄ‚îÄ showcase/             # Main package
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py       # Package exports
‚îÇ   ‚îú‚îÄ‚îÄ builder.py        # Graph builders (loads from YAML)
‚îÇ   ‚îú‚îÄ‚îÄ graph_loader.py   # YAML ‚Üí LangGraph compiler
‚îÇ   ‚îú‚îÄ‚îÄ config.py         # Centralized configuration
‚îÇ   ‚îú‚îÄ‚îÄ executor.py       # YAML prompt executor
‚îÇ   ‚îú‚îÄ‚îÄ cli.py            # CLI commands
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ models/           # Pydantic models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schemas.py    # Output schemas (Analysis, GeneratedContent, etc.)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ state.py      # LangGraph state definition
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ tools/            # Tool execution
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ shell.py      # Shell command executor
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ nodes.py      # Tool node factory
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ agent.py      # Agent node factory
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ storage/          # Persistence layer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ database.py   # SQLite wrapper
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ export.py     # JSON export
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ utils/            # Utilities
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ llm_factory.py # Multi-provider LLM creation
‚îÇ       ‚îî‚îÄ‚îÄ langsmith.py  # Tracing helpers
‚îÇ
‚îú‚îÄ‚îÄ prompts/              # YAML prompt templates
‚îÇ   ‚îú‚îÄ‚îÄ greet.yaml
‚îÇ   ‚îú‚îÄ‚îÄ analyze.yaml
‚îÇ   ‚îú‚îÄ‚îÄ analyze_list.yaml # Jinja2 example with loops/filters
‚îÇ   ‚îú‚îÄ‚îÄ generate.yaml
‚îÇ   ‚îú‚îÄ‚îÄ summarize.yaml
‚îÇ   ‚îî‚îÄ‚îÄ router-demo/      # Tone routing prompts
‚îÇ       ‚îú‚îÄ‚îÄ classify_tone.yaml
‚îÇ       ‚îú‚îÄ‚îÄ respond_positive.yaml
‚îÇ       ‚îú‚îÄ‚îÄ respond_negative.yaml
‚îÇ       ‚îî‚îÄ‚îÄ respond_neutral.yaml
‚îÇ
‚îú‚îÄ‚îÄ tests/                # Test suite
‚îÇ   ‚îú‚îÄ‚îÄ conftest.py       # Shared fixtures
‚îÇ   ‚îú‚îÄ‚îÄ unit/             # Unit tests
‚îÇ   ‚îî‚îÄ‚îÄ integration/      # Integration tests
‚îÇ
‚îî‚îÄ‚îÄ outputs/              # Generated files (gitignored)
```
```

## Pipeline Flow

```mermaid
graph TD
    A["üìù generate"] -->|GeneratedContent| B{should_continue}
    B -->|"‚úì content exists"| C["üîç analyze"]
    B -->|"‚úó error/empty"| F["üõë END"]
    C -->|Analysis| D["üìä summarize"]
    D -->|final_summary| F
    
    style A fill:#e1f5fe
    style C fill:#fff3e0
    style D fill:#e8f5e9
    style F fill:#fce4ec
```

### Node Outputs

| Node | Output Type | Description |
|------|-------------|-------------|
| `generate` | `GeneratedContent` | Title, content, word_count, tags |
| `analyze` | `Analysis` | Summary, key_points, sentiment, confidence |
| `summarize` | `str` | Final combined summary |

### Resume Flow

Pipelines can be resumed from any checkpoint. The resume behavior uses `skip_if_exists`:
nodes check if their output already exists in state and skip LLM calls if so.

```mermaid
graph LR
    subgraph "Resume after 'analyze' completed"
        A1["Load State"] --> B1["analyze (skipped)"] --> C1["summarize"] --> D1["END"]
    end
```

```bash
# Resume an interrupted run
showcase resume --thread-id abc123
```

When resumed:
- Nodes with existing outputs are **skipped** (no duplicate LLM calls)
- Only nodes without outputs in state actually run
- State is preserved via SQLite checkpointing

## Key Patterns

### 1. YAML Prompt Templates

**Simple Templating (Basic Substitution)**:
```yaml
# prompts/generate.yaml
system: |
  You are a creative content writer...

user: |
  Write about: {topic}
  Target length: approximately {word_count} words
```

**Advanced Templating (Jinja2)**:
```yaml
# prompts/analyze_list.yaml
template: |
  Analyze the following {{ items|length }} items:
  
  {% for item in items %}
  ### {{ loop.index }}. {{ item.title }}
  Topic: {{ item.topic }}
  {% if item.tags %}
  Tags: {{ item.tags | join(", ") }}
  {% endif %}
  {% endfor %}
```

**Template Features**:
- **Auto-detection**: Uses Jinja2 if `{{` or `{%` present, otherwise simple formatting
- **Loops**: `{% for item in items %}...{% endfor %}`
- **Conditionals**: `{% if condition %}...{% endif %}`
- **Filters**: `{{ text[:50] }}`, `{{ items | join(", ") }}`, `{{ name | upper }}`
- **Backward compatible**: Existing `{variable}` prompts work unchanged

### 2. Structured Executor

```python
from showcase.executor import execute_prompt
from showcase.models import GeneratedContent

result = execute_prompt(
    "generate",
    variables={"topic": "AI", "word_count": 300},
    output_model=GeneratedContent,
)
print(result.title)  # Typed access!
```

### 3. Multi-Provider LLM Support

```python
from showcase.executor import execute_prompt

# Use default provider (Anthropic)
result = execute_prompt(
    "greet",
    variables={"name": "Alice", "style": "formal"},
)

# Switch to Mistral
result = execute_prompt(
    "greet",
    variables={"name": "Bob", "style": "casual"},
    provider="mistral",
)

# Or set via environment variable
# PROVIDER=openai showcase run ...
```

Supported providers:
- **Anthropic** (default): Claude models
- **Mistral**: Mistral Large and other models  
- **OpenAI**: GPT-4 and other models

Provider selection priority:
1. Function parameter: `execute_prompt(..., provider="mistral")`
2. YAML metadata: `provider: mistral` in prompt file
3. Environment variable: `PROVIDER=mistral`
4. Default: `anthropic`

### 4. YAML Graph Configuration

Pipelines are defined declaratively in YAML and compiled to LangGraph:

```yaml
# graphs/showcase.yaml
version: "1.0"
name: showcase
description: Content generation pipeline

state_class: showcase.models.ShowcaseState

defaults:
  provider: mistral
  temperature: 0.7

nodes:
  generate:
    type: llm
    prompt: generate
    output_model: showcase.models.GeneratedContent
    temperature: 0.8
    variables:
      topic: "{state.topic}"
      word_count: "{state.word_count}"
      style: "{state.style}"
    state_key: generated

  analyze:
    type: llm
    prompt: analyze
    output_model: showcase.models.Analysis
    temperature: 0.3
    variables:
      content: "{state.generated.content}"
    state_key: analysis
    requires: [generated]

  summarize:
    type: llm
    prompt: summarize
    temperature: 0.5
    state_key: final_summary
    requires: [generated, analysis]

edges:
  - from: START
    to: generate
  - from: generate
    to: analyze
    condition: continue
  - from: generate
    to: END
    condition: end
  - from: analyze
    to: summarize
  - from: summarize
    to: END
```

**Load and run**:
```python
from showcase.builder import build_showcase_graph

graph = build_showcase_graph().compile()  # Loads from graphs/showcase.yaml
result = graph.invoke(initial_state)
```

### 5. State Persistence

```python
from showcase.storage import ShowcaseDB

db = ShowcaseDB()
db.save_state("thread-123", state)
state = db.load_state("thread-123")
```

### 6. LangSmith Tracing

```python
from showcase.langsmith_utils import print_run_tree

print_run_tree(verbose=True)
# üìä Execution Tree:
# ‚îî‚îÄ showcase_graph (12.3s) ‚úÖ
#    ‚îú‚îÄ generate (5.2s) ‚úÖ
#    ‚îú‚îÄ analyze (3.1s) ‚úÖ
#    ‚îî‚îÄ summarize (4.0s) ‚úÖ
```

### 7. Shell Tools & Agent Nodes

Define shell tools and let the LLM decide when to use them:

```yaml
# graphs/git-report.yaml
tools:
  recent_commits:
    type: shell
    command: git log --oneline -n {count}
    description: "List recent commits"
    
  changed_files:
    type: shell
    command: git diff --name-only HEAD~{n}
    description: "List files changed in last n commits"

nodes:
  analyze:
    type: agent              # LLM decides which tools to call
    prompt: git_analyst
    tools: [recent_commits, changed_files]
    max_iterations: 8
    state_key: analysis
```

Run the git analysis agent:

```bash
showcase git-report -q "What changed recently?"
showcase git-report -q "Summarize the test directory"
```

**Node types:**
- `type: llm` - Standard LLM call with structured output
- `type: router` - Classify and route to different paths
- `type: tool` - Execute a specific shell tool
- `type: agent` - LLM loop that autonomously calls tools

## Environment Variables

| Variable | Required | Description |
|----------|----------|-------------|
| `ANTHROPIC_API_KEY` | Yes* | Anthropic API key (* if using Anthropic) |
| `MISTRAL_API_KEY` | No | Mistral API key (required if using Mistral) |
| `OPENAI_API_KEY` | No | OpenAI API key (required if using OpenAI) |
| `PROVIDER` | No | Default LLM provider (anthropic/mistral/openai) |
| `ANTHROPIC_MODEL` | No | Anthropic model (default: claude-sonnet-4-20250514) |
| `MISTRAL_MODEL` | No | Mistral model (default: mistral-large-latest) |
| `OPENAI_MODEL` | No | OpenAI model (default: gpt-4o) |
| `LANGCHAIN_TRACING` | No | Enable LangSmith tracing |
| `LANGCHAIN_API_KEY` | No | LangSmith API key |
| `LANGCHAIN_ENDPOINT` | No | LangSmith endpoint URL |
| `LANGCHAIN_PROJECT` | No | LangSmith project name |

## Testing

Run the test suite:

```bash
# Run all tests
pytest tests/ -v

# Run only unit tests
pytest tests/unit/ -v

# Run only integration tests
pytest tests/integration/ -v

# Run with coverage report
pytest tests/ --cov=showcase --cov-report=term-missing

# Run with HTML coverage report
pytest tests/ --cov=showcase --cov-report=html
# Then open htmlcov/index.html
```

**Current coverage**: 60% overall, 98% on graph_loader, 100% on builder/llm_factory.

## Extending the Pipeline

### Adding a New Node (YAML-First Approach)

Let's add a "fact_check" node that verifies generated content:

**Step 1: Define the output schema** (`showcase/models/schemas.py`):
```python
class FactCheck(BaseModel):
    """Structured fact-checking output."""
    
    claims: list[str] = Field(description="Claims identified in content")
    verified: bool = Field(description="Whether claims are verifiable")
    confidence: float = Field(ge=0.0, le=1.0, description="Verification confidence")
    notes: str = Field(description="Additional context")
```

**Step 2: Create the prompt** (`prompts/fact_check.yaml`):
```yaml
system: |
  You are a fact-checker. Analyze the given content and identify
  claims that can be verified. Assess the overall verifiability.

user: |
  Content to fact-check:
  {content}
  
  Identify key claims and assess their verifiability.
```

**Step 3: Add to state** (`showcase/models/state.py`):
```python
class ShowcaseState(TypedDict, total=False):
    # ... existing fields ...
    fact_check: FactCheck | None  # Add new field
```

**Step 4: Add the node to your graph** (`graphs/showcase.yaml`):
```yaml
nodes:
  generate:
    type: prompt
    prompt: generate
    output_model: showcase.models.GeneratedContent
    variables:
      topic: topic
    output_key: generated

  fact_check:  # ‚ú® New node - just YAML!
    type: prompt
    prompt: fact_check
    output_model: showcase.models.FactCheck
    requires: [generated]
    variables:
      content: generated.content
    output_key: fact_check

  analyze:
    # ... existing config ...

edges:
  - from: START
    to: generate
  - from: generate
    to: fact_check
    condition:
      type: has_value
      field: generated
  - from: fact_check
    to: analyze
  # ... rest of edges ...
```

That's it! No Python node code needed. The graph loader dynamically generates the node function.

Resulting pipeline:
```mermaid
graph TD
    A[generate] --> B{has generated?}
    B -->|yes| C[fact_check]
    C --> D[analyze]
    D --> E[summarize]
    E --> F[END]
    B -->|no| F
```

### Adding Conditional Branching

Route to different nodes based on analysis results (all in YAML):

```yaml
edges:
  - from: analyze
    to: rewrite_node
    condition:
      type: field_equals
      field: analysis.sentiment
      value: negative
  
  - from: analyze
    to: enhance_node
    condition:
      type: field_equals
      field: analysis.sentiment
      value: positive
  
  - from: analyze
    to: summarize  # Default fallback
```

### Add a New Prompt

1. Create `prompts/new_prompt.yaml`:
```yaml
system: Your system prompt...
user: Your user prompt with {variables}...
```

2. Call it:
```python
result = execute_prompt("new_prompt", variables={"var": "value"})
```

### Add Structured Output

1. Define model in `showcase/models/schemas.py`:
```python
class MyOutput(BaseModel):
    field: str = Field(description="...")
```

2. Use with executor:
```python
result = execute_prompt("prompt", output_model=MyOutput)
```

## Known Issues & Future Improvements

This project demonstrates solid production patterns with declarative YAML-based configuration.

### Completed Features

| Feature | Status | Notes |
|---------|--------|-------|
| YAML Graph Configuration | ‚úÖ | Declarative pipeline definition in `graphs/showcase.yaml` |
| Jinja2 Templating | ‚úÖ | Hybrid auto-detection (simple {var} + advanced Jinja2) |
| Multi-Provider LLMs | ‚úÖ | Factory pattern supporting Anthropic/Mistral/OpenAI |
| Dynamic Node Generation | ‚úÖ | Nodes compiled from YAML at runtime |

### Implemented Patterns

| Feature | Status | Notes |
|---------|--------|-------|
| Branching/Routing | ‚úÖ | `type: router` for LLM-based conditional routing |
| Self-Correction Loops | ‚úÖ | Reflexion pattern with critique ‚Üí refine cycles |
| Tool/Agent Patterns | ‚úÖ | Shell tools + agent nodes with LangChain tool binding |
| Per-Node Error Handling | ‚úÖ | `on_error: skip/retry/fail/fallback` |
| Conversation Memory | ‚úÖ | Message accumulation via `AgentState.messages` |
| Native Checkpointing | ‚úÖ | `SqliteSaver` from `langgraph-checkpoint-sqlite` |
| State Export | ‚úÖ | JSON/Markdown export with `export_result()` |
| LangSmith Share Links | ‚úÖ | Auto-generate public trace URLs after runs |

### Missing LangGraph Features

| Feature | Status | Notes |
|---------|--------|-------|
| Parallel Nodes | ‚ùå | No concurrent execution of independent nodes |
| Human-in-the-Loop | ‚ùå | No `interrupt_before` / `interrupt_after` demonstration |
| Streaming | ‚ùå | No streaming output support |
| Sub-graphs | ‚ùå | No nested graph composition |
| Async Nodes | ‚ùå | Everything is synchronous |

### Potential Enhancements

1. **Add parallel nodes** - Run `sentiment_analysis` and `topic_extraction` concurrently
2. **Use LangGraph's checkpointer** - Replace custom DB with native persistence
3. **Add streaming** - `--stream` CLI flag for real-time output
4. **JSON Schema validation** - Validate `graphs/*.yaml` against schema

## License

MIT

## Remember

Prompts in yaml templates, graphs in yaml, shared executor, pydantic, data stored in sqlite, langgraph, langsmith, venv, tdd red-green-refactor, modules < 400 lines, kiss
