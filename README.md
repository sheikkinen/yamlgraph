# LangGraph Showcase App

A minimal, self-contained demonstration of a production-ready LLM pipeline using:

- **YAML Prompts** - Declarative prompt templates
- **Pydantic Models** - Structured LLM outputs
- **LangGraph** - Pipeline orchestration
- **SQLite** - State persistence
- **LangSmith** - Observability and tracing
- **JSON Export** - Result serialization

## Quick Start

### 1. Setup Environment

```bash
# Clone or copy the showcase directory
cd showcase

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# Install as editable package
pip install -e .

# Configure environment
cp .env.sample .env
# Edit .env with your ANTHROPIC_API_KEY
```

### 2. Run the Pipeline

```bash
# Using the CLI command (after pip install -e .)
showcase run --topic "machine learning" --style casual

# Or using python -m
python -m showcase.cli run --topic "machine learning" --style casual

# With export
showcase run -t "climate change" -s informative -w 500 --export

# View recent runs
showcase list-runs

# Resume a run
showcase resume --thread-id abc123

# View execution trace (requires LangSmith)
showcase trace --verbose
```

## Architecture

```
showcase/
â”œâ”€â”€ README.md
â”œâ”€â”€ pyproject.toml        # Package definition with CLI entry point
â”œâ”€â”€ requirements.txt      # Dependencies
â”œâ”€â”€ .env.sample           # Environment template
â”œâ”€â”€ run.py                # Simple entry point
â”‚
â”œâ”€â”€ showcase/             # Main package
â”‚   â”œâ”€â”€ __init__.py       # Package exports
â”‚   â”œâ”€â”€ builder.py        # Graph builders + pipeline runner
â”‚   â”œâ”€â”€ config.py         # Centralized configuration
â”‚   â”œâ”€â”€ executor.py       # YAML prompt executor
â”‚   â”œâ”€â”€ cli.py            # CLI commands
â”‚   â”‚
â”‚   â”œâ”€â”€ models/           # Pydantic models
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ schemas.py    # Output schemas (Analysis, GeneratedContent, etc.)
â”‚   â”‚   â””â”€â”€ state.py      # LangGraph state definition
â”‚   â”‚
â”‚   â”œâ”€â”€ nodes/            # Graph node functions
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ content.py    # generate, analyze, summarize nodes
â”‚   â”‚
â”‚   â”œâ”€â”€ storage/          # Persistence layer
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ database.py   # SQLite wrapper
â”‚   â”‚   â””â”€â”€ export.py     # JSON export
â”‚   â”‚
â”‚   â””â”€â”€ utils/            # Utilities
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ langsmith.py  # Tracing helpers
â”‚
â”œâ”€â”€ prompts/              # YAML prompt templates
â”‚   â”œâ”€â”€ greet.yaml
â”‚   â”œâ”€â”€ analyze.yaml
â”‚   â”œâ”€â”€ generate.yaml
â”‚   â””â”€â”€ summarize.yaml
â”‚
â”œâ”€â”€ tests/                # Test suite
â”‚   â”œâ”€â”€ conftest.py       # Shared fixtures
â”‚   â”œâ”€â”€ unit/             # Unit tests
â”‚   â””â”€â”€ integration/      # Integration tests
â”‚
â””â”€â”€ outputs/              # Generated files (gitignored)
```

## Pipeline Flow

```mermaid
graph TD
    A["ðŸ“ generate"] -->|GeneratedContent| B{should_continue}
    B -->|"âœ“ content exists"| C["ðŸ” analyze"]
    B -->|"âœ— error/empty"| F["ðŸ›‘ END"]
    C -->|Analysis| D["ðŸ“Š summarize"]
    D -->|final_summary| F
    
    style A fill:#e1f5fe
    style C fill:#fff3e0
    style D fill:#e8f5e9
    style F fill:#fce4ec
```

### Node Outputs

| Node | Output Type | Description |
|------|-------------|-------------|
| `generate` | `GeneratedContent` | Title, content, word_count, tags |
| `analyze` | `Analysis` | Summary, key_points, sentiment, confidence |
| `summarize` | `str` | Final combined summary |

### Resume Flow

Pipelines can be resumed from any checkpoint:

```mermaid
graph LR
    subgraph "Resume from analyze"
        A1["Load State"] --> B1["analyze"] --> C1["summarize"] --> D1["END"]
    end
    subgraph "Resume from summarize"
        A2["Load State"] --> C2["summarize"] --> D2["END"]
    end
```

```bash
# Resume an interrupted run
showcase resume --thread-id abc123
```

## Key Patterns

### 1. YAML Prompt Templates

```yaml
# prompts/generate.yaml
system: |
  You are a creative content writer...

user: |
  Write about: {topic}
  Target length: approximately {word_count} words
```

### 2. Structured Executor

```python
from showcase.executor import execute_prompt
from showcase.models import GeneratedContent

result = execute_prompt(
    "generate",
    variables={"topic": "AI", "word_count": 300},
    output_model=GeneratedContent,
)
print(result.title)  # Typed access!
```

### 3. LangGraph Pipeline

```python
from showcase.graph import build_showcase_graph

graph = build_showcase_graph().compile()
result = graph.invoke(initial_state)
```

### 4. State Persistence

```python
from showcase.storage import ShowcaseDB

db = ShowcaseDB()
db.save_state("thread-123", state)
state = db.load_state("thread-123")
```

### 5. LangSmith Tracing

```python
from showcase.langsmith_utils import print_run_tree

print_run_tree(verbose=True)
# ðŸ“Š Execution Tree:
# â””â”€ showcase_graph (12.3s) âœ…
#    â”œâ”€ generate (5.2s) âœ…
#    â”œâ”€ analyze (3.1s) âœ…
#    â””â”€ summarize (4.0s) âœ…
```

## Environment Variables

| Variable | Required | Description |
|----------|----------|-------------|
| `ANTHROPIC_API_KEY` | Yes | Anthropic API key |
| `ANTHROPIC_MODEL` | No | Model name (default: claude-sonnet-4-20250514) |
| `LANGCHAIN_TRACING_V2` | No | Enable LangSmith tracing |
| `LANGCHAIN_API_KEY` | No | LangSmith API key |
| `LANGCHAIN_ENDPOINT` | No | LangSmith endpoint URL |
| `LANGCHAIN_PROJECT` | No | LangSmith project name |

## Testing

Run the test suite:

```bash
# Run all tests
pytest tests/ -v

# Run only unit tests
pytest tests/unit/ -v

# Run only integration tests
pytest tests/integration/ -v

# Run with coverage
pytest tests/ --cov=showcase --cov-report=term-missing
```

## Extending the Pipeline

### Adding a New Node (Complete Example)

Let's add a "fact_check" node that verifies generated content:

**Step 1: Define the output schema** (`showcase/models/schemas.py`):
```python
class FactCheck(BaseModel):
    """Structured fact-checking output."""
    
    claims: list[str] = Field(description="Claims identified in content")
    verified: bool = Field(description="Whether claims are verifiable")
    confidence: float = Field(ge=0.0, le=1.0, description="Verification confidence")
    notes: str = Field(description="Additional context")
```

**Step 2: Create the prompt** (`prompts/fact_check.yaml`):
```yaml
system: |
  You are a fact-checker. Analyze the given content and identify
  claims that can be verified. Assess the overall verifiability.

user: |
  Content to fact-check:
  {content}
  
  Identify key claims and assess their verifiability.
```

**Step 3: Add the node function** (`showcase/nodes/content.py`):
```python
from showcase.models import FactCheck

def fact_check_node(state: ShowcaseState) -> dict:
    """Fact-check the generated content."""
    generated = state.get("generated")
    if not generated:
        error = PipelineError(
            type=ErrorType.STATE_ERROR,
            message="No content to fact-check",
            node="fact_check",
            retryable=False,
        )
        return {**_add_error(state, error), "current_step": "fact_check"}
    
    print(f"ðŸ”Ž Fact-checking: {generated.title}")
    
    try:
        result = execute_prompt(
            "fact_check",
            variables={"content": generated.content},
            output_model=FactCheck,
            temperature=0.2,  # Low temp for accuracy
        )
        print(f"   âœ“ Verified: {result.verified} (confidence: {result.confidence:.2f})")
        return {"fact_check": result, "current_step": "fact_check"}
    except Exception as e:
        error = PipelineError.from_exception(e, node="fact_check")
        return {**_add_error(state, error), "current_step": "fact_check"}
```

**Step 4: Add to state** (`showcase/models/state.py`):
```python
class ShowcaseState(TypedDict, total=False):
    # ... existing fields ...
    fact_check: FactCheck | None  # Add new field
```

**Step 5: Wire into the graph** (`showcase/builder.py`):
```python
from showcase.nodes import fact_check_node

def build_showcase_graph() -> StateGraph:
    graph = StateGraph(ShowcaseState)
    
    graph.add_node("generate", generate_node)
    graph.add_node("fact_check", fact_check_node)  # New node
    graph.add_node("analyze", analyze_node)
    graph.add_node("summarize", summarize_node)
    
    graph.set_entry_point("generate")
    graph.add_conditional_edges("generate", should_continue, {
        "continue": "fact_check",  # Route to fact_check first
        "end": END,
    })
    graph.add_edge("fact_check", "analyze")  # Then to analyze
    graph.add_edge("analyze", "summarize")
    graph.add_edge("summarize", END)
    
    return graph
```

Resulting pipeline:
```mermaid
graph TD
    A[generate] --> B{should_continue}
    B -->|continue| C[fact_check]
    C --> D[analyze]
    D --> E[summarize]
    E --> F[END]
    B -->|end| F
```

### Adding Conditional Branching

Route to different nodes based on analysis results:

```python
def route_by_sentiment(state: ShowcaseState) -> str:
    """Route based on sentiment analysis."""
    analysis = state.get("analysis")
    if not analysis:
        return "default"
    
    if analysis.sentiment == "negative" and analysis.confidence > 0.8:
        return "handle_negative"
    elif analysis.sentiment == "positive":
        return "celebrate"
    return "default"

# In build_showcase_graph():
graph.add_conditional_edges(
    "analyze",
    route_by_sentiment,
    {
        "handle_negative": "rewrite_node",
        "celebrate": "enhance_node", 
        "default": "summarize",
    }
)
```

### Add a New Prompt

1. Create `prompts/new_prompt.yaml`:
```yaml
system: Your system prompt...
user: Your user prompt with {variables}...
```

2. Call it:
```python
result = execute_prompt("new_prompt", variables={"var": "value"})
```

### Add Structured Output

1. Define model in `showcase/models/schemas.py`:
```python
class MyOutput(BaseModel):
    field: str = Field(description="...")
```

2. Use with executor:
```python
result = execute_prompt("prompt", output_model=MyOutput)
```

## License

MIT

## Remember

Prompts in yaml templates, shared executor, pydantic, data stored in sqllite, langgraph, langsmith, venv, tdd red-green-blue, refactor modules to < 400 lines, kiss
